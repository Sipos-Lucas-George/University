{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8210b19",
   "metadata": {},
   "source": [
    "## A.I. Assignment 5\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Get more familiar with tensors in pytorch \n",
    "* Create a simple multilayer perceptron model with pytorch\n",
    "* Visualise the parameters\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "Build a fully connected feed forward network that adds two bits. Determine the a propper achitecture for this network (what database you use for this problem? how many layers? how many neurons on each layer? what is the activation function? what is the loss function? etc)\n",
    "\n",
    "Create at least 3 such networks and compare their performance (how accurate they are?, how farst they are trained to get at 1 accuracy?)\n",
    "\n",
    "Display for the best one the weights for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3614e5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.248569Z",
     "end_time": "2023-04-01T19:43:02.309015Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ee7e7d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.297922Z",
     "end_time": "2023-04-01T19:43:02.309700Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#model1 = nn.Sequential(OrderedDict([\n",
    "#    ('hidden', nn.\n",
    "#]))\n",
    "\n",
    "model1 = nn.Sequential(OrderedDict([\n",
    "    ('hidden', nn.Linear(2, 2)),\n",
    "    ('output', nn.Linear(2, 2))\n",
    "]))\n",
    "\n",
    "model2 = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(2, 4)),\n",
    "    ('tanh1', nn.Tanh()),\n",
    "    ('fc2', nn.Linear(4, 4)),\n",
    "    ('tanh2', nn.Tanh()),\n",
    "    ('fc3', nn.Linear(4, 2)),\n",
    "    ('tanh3', nn.Tanh())\n",
    "]))\n",
    "\n",
    "model3 = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(2, 3)),\n",
    "    ('sigmoid1', nn.Sigmoid()),\n",
    "    ('fc3', nn.Linear(3, 2)),\n",
    "    ('sigmoid2', nn.Sigmoid())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "665ae958",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.298054Z",
     "end_time": "2023-04-01T19:43:02.310352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (hidden): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (output): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (tanh2): Tanh()\n",
      "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (tanh3): Tanh()\n",
      ")\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (fc3): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (sigmoid2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e26f0d3e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.298275Z",
     "end_time": "2023-04-01T19:43:02.310754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "#data_in = torch.tensor( ...\n",
    "data_in = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "\n",
    "print(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb16bbc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.298403Z",
     "end_time": "2023-04-01T19:43:02.311149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# data_target = torch.tensor( ...\n",
    "data_target = torch.tensor([[0, 0], [0, 1], [0, 1], [1, 0]], dtype=torch.float32)\n",
    "\n",
    "print(data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69d920ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.298483Z",
     "end_time": "2023-04-01T19:43:02.311385Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# criterion = \n",
    "# optimizer =\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.1)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.1)\n",
    "optimizer3 = torch.optim.SGD(model3.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde91f6f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:02.345573Z",
     "end_time": "2023-04-01T19:43:10.101471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Epoch [100/10000], Loss: 0.1609\n",
      "Epoch [200/10000], Loss: 0.1565\n",
      "Epoch [300/10000], Loss: 0.1563\n",
      "Epoch [400/10000], Loss: 0.1563\n",
      "Epoch [500/10000], Loss: 0.1563\n",
      "Epoch [600/10000], Loss: 0.1563\n",
      "Epoch [700/10000], Loss: 0.1563\n",
      "Epoch [800/10000], Loss: 0.1563\n",
      "Epoch [900/10000], Loss: 0.1563\n",
      "Epoch [1000/10000], Loss: 0.1563\n",
      "Epoch [1100/10000], Loss: 0.1563\n",
      "Epoch [1200/10000], Loss: 0.1563\n",
      "Epoch [1300/10000], Loss: 0.1563\n",
      "Epoch [1400/10000], Loss: 0.1563\n",
      "Epoch [1500/10000], Loss: 0.1563\n",
      "Epoch [1600/10000], Loss: 0.1563\n",
      "Epoch [1700/10000], Loss: 0.1563\n",
      "Epoch [1800/10000], Loss: 0.1562\n",
      "Epoch [1900/10000], Loss: 0.1562\n",
      "Epoch [2000/10000], Loss: 0.1562\n",
      "Epoch [2100/10000], Loss: 0.1562\n",
      "Epoch [2200/10000], Loss: 0.1562\n",
      "Epoch [2300/10000], Loss: 0.1562\n",
      "Epoch [2400/10000], Loss: 0.1562\n",
      "Epoch [2500/10000], Loss: 0.1562\n",
      "Epoch [2600/10000], Loss: 0.1562\n",
      "Epoch [2700/10000], Loss: 0.1562\n",
      "Epoch [2800/10000], Loss: 0.1562\n",
      "Epoch [2900/10000], Loss: 0.1562\n",
      "Epoch [3000/10000], Loss: 0.1562\n",
      "Epoch [3100/10000], Loss: 0.1562\n",
      "Epoch [3200/10000], Loss: 0.1562\n",
      "Epoch [3300/10000], Loss: 0.1562\n",
      "Epoch [3400/10000], Loss: 0.1562\n",
      "Epoch [3500/10000], Loss: 0.1562\n",
      "Epoch [3600/10000], Loss: 0.1562\n",
      "Epoch [3700/10000], Loss: 0.1562\n",
      "Epoch [3800/10000], Loss: 0.1562\n",
      "Epoch [3900/10000], Loss: 0.1562\n",
      "Epoch [4000/10000], Loss: 0.1562\n",
      "Epoch [4100/10000], Loss: 0.1562\n",
      "Epoch [4200/10000], Loss: 0.1562\n",
      "Epoch [4300/10000], Loss: 0.1562\n",
      "Epoch [4400/10000], Loss: 0.1562\n",
      "Epoch [4500/10000], Loss: 0.1562\n",
      "Epoch [4600/10000], Loss: 0.1562\n",
      "Epoch [4700/10000], Loss: 0.1562\n",
      "Epoch [4800/10000], Loss: 0.1562\n",
      "Epoch [4900/10000], Loss: 0.1562\n",
      "Epoch [5000/10000], Loss: 0.1562\n",
      "Epoch [5100/10000], Loss: 0.1562\n",
      "Epoch [5200/10000], Loss: 0.1562\n",
      "Epoch [5300/10000], Loss: 0.1562\n",
      "Epoch [5400/10000], Loss: 0.1562\n",
      "Epoch [5500/10000], Loss: 0.1562\n",
      "Epoch [5600/10000], Loss: 0.1562\n",
      "Epoch [5700/10000], Loss: 0.1562\n",
      "Epoch [5800/10000], Loss: 0.1562\n",
      "Epoch [5900/10000], Loss: 0.1562\n",
      "Epoch [6000/10000], Loss: 0.1562\n",
      "Epoch [6100/10000], Loss: 0.1562\n",
      "Epoch [6200/10000], Loss: 0.1562\n",
      "Epoch [6300/10000], Loss: 0.1562\n",
      "Epoch [6400/10000], Loss: 0.1562\n",
      "Epoch [6500/10000], Loss: 0.1562\n",
      "Epoch [6600/10000], Loss: 0.1562\n",
      "Epoch [6700/10000], Loss: 0.1562\n",
      "Epoch [6800/10000], Loss: 0.1562\n",
      "Epoch [6900/10000], Loss: 0.1562\n",
      "Epoch [7000/10000], Loss: 0.1562\n",
      "Epoch [7100/10000], Loss: 0.1562\n",
      "Epoch [7200/10000], Loss: 0.1562\n",
      "Epoch [7300/10000], Loss: 0.1562\n",
      "Epoch [7400/10000], Loss: 0.1562\n",
      "Epoch [7500/10000], Loss: 0.1562\n",
      "Epoch [7600/10000], Loss: 0.1562\n",
      "Epoch [7700/10000], Loss: 0.1562\n",
      "Epoch [7800/10000], Loss: 0.1562\n",
      "Epoch [7900/10000], Loss: 0.1562\n",
      "Epoch [8000/10000], Loss: 0.1562\n",
      "Epoch [8100/10000], Loss: 0.1562\n",
      "Epoch [8200/10000], Loss: 0.1562\n",
      "Epoch [8300/10000], Loss: 0.1562\n",
      "Epoch [8400/10000], Loss: 0.1562\n",
      "Epoch [8500/10000], Loss: 0.1562\n",
      "Epoch [8600/10000], Loss: 0.1562\n",
      "Epoch [8700/10000], Loss: 0.1562\n",
      "Epoch [8800/10000], Loss: 0.1562\n",
      "Epoch [8900/10000], Loss: 0.1562\n",
      "Epoch [9000/10000], Loss: 0.1562\n",
      "Epoch [9100/10000], Loss: 0.1562\n",
      "Epoch [9200/10000], Loss: 0.1562\n",
      "Epoch [9300/10000], Loss: 0.1562\n",
      "Epoch [9400/10000], Loss: 0.1562\n",
      "Epoch [9500/10000], Loss: 0.1562\n",
      "Epoch [9600/10000], Loss: 0.1562\n",
      "Epoch [9700/10000], Loss: 0.1562\n",
      "Epoch [9800/10000], Loss: 0.1562\n",
      "Epoch [9900/10000], Loss: 0.1562\n",
      "Epoch [10000/10000], Loss: 0.1562\n",
      "Model 2\n",
      "Epoch [100/10000], Loss: 0.1666\n",
      "Epoch [200/10000], Loss: 0.1305\n",
      "Epoch [300/10000], Loss: 0.0953\n",
      "Epoch [400/10000], Loss: 0.0523\n",
      "Epoch [500/10000], Loss: 0.0184\n",
      "Epoch [600/10000], Loss: 0.0077\n",
      "Epoch [700/10000], Loss: 0.0043\n",
      "Epoch [800/10000], Loss: 0.0029\n",
      "Epoch [900/10000], Loss: 0.0021\n",
      "Epoch [1000/10000], Loss: 0.0016\n",
      "Epoch [1100/10000], Loss: 0.0013\n",
      "Epoch [1200/10000], Loss: 0.0011\n",
      "Epoch [1300/10000], Loss: 0.0010\n",
      "Epoch [1400/10000], Loss: 0.0008\n",
      "Epoch [1500/10000], Loss: 0.0007\n",
      "Epoch [1600/10000], Loss: 0.0007\n",
      "Epoch [1700/10000], Loss: 0.0006\n",
      "Epoch [1800/10000], Loss: 0.0006\n",
      "Epoch [1900/10000], Loss: 0.0005\n",
      "Epoch [2000/10000], Loss: 0.0005\n",
      "Epoch [2100/10000], Loss: 0.0004\n",
      "Epoch [2200/10000], Loss: 0.0004\n",
      "Epoch [2300/10000], Loss: 0.0004\n",
      "Epoch [2400/10000], Loss: 0.0004\n",
      "Epoch [2500/10000], Loss: 0.0003\n",
      "Epoch [2600/10000], Loss: 0.0003\n",
      "Epoch [2700/10000], Loss: 0.0003\n",
      "Epoch [2800/10000], Loss: 0.0003\n",
      "Epoch [2900/10000], Loss: 0.0003\n",
      "Epoch [3000/10000], Loss: 0.0003\n",
      "Epoch [3100/10000], Loss: 0.0003\n",
      "Epoch [3200/10000], Loss: 0.0002\n",
      "Epoch [3300/10000], Loss: 0.0002\n",
      "Epoch [3400/10000], Loss: 0.0002\n",
      "Epoch [3500/10000], Loss: 0.0002\n",
      "Epoch [3600/10000], Loss: 0.0002\n",
      "Epoch [3700/10000], Loss: 0.0002\n",
      "Epoch [3800/10000], Loss: 0.0002\n",
      "Epoch [3900/10000], Loss: 0.0002\n",
      "Epoch [4000/10000], Loss: 0.0002\n",
      "Epoch [4100/10000], Loss: 0.0002\n",
      "Epoch [4200/10000], Loss: 0.0002\n",
      "Epoch [4300/10000], Loss: 0.0002\n",
      "Epoch [4400/10000], Loss: 0.0002\n",
      "Epoch [4500/10000], Loss: 0.0002\n",
      "Epoch [4600/10000], Loss: 0.0002\n",
      "Epoch [4700/10000], Loss: 0.0002\n",
      "Epoch [4800/10000], Loss: 0.0001\n",
      "Epoch [4900/10000], Loss: 0.0001\n",
      "Epoch [5000/10000], Loss: 0.0001\n",
      "Epoch [5100/10000], Loss: 0.0001\n",
      "Epoch [5200/10000], Loss: 0.0001\n",
      "Epoch [5300/10000], Loss: 0.0001\n",
      "Epoch [5400/10000], Loss: 0.0001\n",
      "Epoch [5500/10000], Loss: 0.0001\n",
      "Epoch [5600/10000], Loss: 0.0001\n",
      "Epoch [5700/10000], Loss: 0.0001\n",
      "Epoch [5800/10000], Loss: 0.0001\n",
      "Epoch [5900/10000], Loss: 0.0001\n",
      "Epoch [6000/10000], Loss: 0.0001\n",
      "Epoch [6100/10000], Loss: 0.0001\n",
      "Epoch [6200/10000], Loss: 0.0001\n",
      "Epoch [6300/10000], Loss: 0.0001\n",
      "Epoch [6400/10000], Loss: 0.0001\n",
      "Epoch [6500/10000], Loss: 0.0001\n",
      "Epoch [6600/10000], Loss: 0.0001\n",
      "Epoch [6700/10000], Loss: 0.0001\n",
      "Epoch [6800/10000], Loss: 0.0001\n",
      "Epoch [6900/10000], Loss: 0.0001\n",
      "Epoch [7000/10000], Loss: 0.0001\n",
      "Epoch [7100/10000], Loss: 0.0001\n",
      "Epoch [7200/10000], Loss: 0.0001\n",
      "Epoch [7300/10000], Loss: 0.0001\n",
      "Epoch [7400/10000], Loss: 0.0001\n",
      "Epoch [7500/10000], Loss: 0.0001\n",
      "Epoch [7600/10000], Loss: 0.0001\n",
      "Epoch [7700/10000], Loss: 0.0001\n",
      "Epoch [7800/10000], Loss: 0.0001\n",
      "Epoch [7900/10000], Loss: 0.0001\n",
      "Epoch [8000/10000], Loss: 0.0001\n",
      "Epoch [8100/10000], Loss: 0.0001\n",
      "Epoch [8200/10000], Loss: 0.0001\n",
      "Epoch [8300/10000], Loss: 0.0001\n",
      "Epoch [8400/10000], Loss: 0.0001\n",
      "Epoch [8500/10000], Loss: 0.0001\n",
      "Epoch [8600/10000], Loss: 0.0001\n",
      "Epoch [8700/10000], Loss: 0.0001\n",
      "Epoch [8800/10000], Loss: 0.0001\n",
      "Epoch [8900/10000], Loss: 0.0001\n",
      "Epoch [9000/10000], Loss: 0.0001\n",
      "Epoch [9100/10000], Loss: 0.0001\n",
      "Epoch [9200/10000], Loss: 0.0001\n",
      "Epoch [9300/10000], Loss: 0.0001\n",
      "Epoch [9400/10000], Loss: 0.0001\n",
      "Epoch [9500/10000], Loss: 0.0001\n",
      "Epoch [9600/10000], Loss: 0.0001\n",
      "Epoch [9700/10000], Loss: 0.0001\n",
      "Epoch [9800/10000], Loss: 0.0001\n",
      "Epoch [9900/10000], Loss: 0.0001\n",
      "Epoch [10000/10000], Loss: 0.0001\n",
      "Model 3\n",
      "Epoch [100/10000], Loss: 0.2256\n",
      "Epoch [200/10000], Loss: 0.2214\n",
      "Epoch [300/10000], Loss: 0.2202\n",
      "Epoch [400/10000], Loss: 0.2195\n",
      "Epoch [500/10000], Loss: 0.2187\n",
      "Epoch [600/10000], Loss: 0.2180\n",
      "Epoch [700/10000], Loss: 0.2172\n",
      "Epoch [800/10000], Loss: 0.2163\n",
      "Epoch [900/10000], Loss: 0.2154\n",
      "Epoch [1000/10000], Loss: 0.2144\n",
      "Epoch [1100/10000], Loss: 0.2133\n",
      "Epoch [1200/10000], Loss: 0.2121\n",
      "Epoch [1300/10000], Loss: 0.2108\n",
      "Epoch [1400/10000], Loss: 0.2094\n",
      "Epoch [1500/10000], Loss: 0.2079\n",
      "Epoch [1600/10000], Loss: 0.2064\n",
      "Epoch [1700/10000], Loss: 0.2048\n",
      "Epoch [1800/10000], Loss: 0.2032\n",
      "Epoch [1900/10000], Loss: 0.2015\n",
      "Epoch [2000/10000], Loss: 0.1999\n",
      "Epoch [2100/10000], Loss: 0.1982\n",
      "Epoch [2200/10000], Loss: 0.1965\n",
      "Epoch [2300/10000], Loss: 0.1948\n",
      "Epoch [2400/10000], Loss: 0.1932\n",
      "Epoch [2500/10000], Loss: 0.1915\n",
      "Epoch [2600/10000], Loss: 0.1898\n",
      "Epoch [2700/10000], Loss: 0.1881\n",
      "Epoch [2800/10000], Loss: 0.1864\n",
      "Epoch [2900/10000], Loss: 0.1847\n",
      "Epoch [3000/10000], Loss: 0.1830\n",
      "Epoch [3100/10000], Loss: 0.1814\n",
      "Epoch [3200/10000], Loss: 0.1797\n",
      "Epoch [3300/10000], Loss: 0.1780\n",
      "Epoch [3400/10000], Loss: 0.1764\n",
      "Epoch [3500/10000], Loss: 0.1748\n",
      "Epoch [3600/10000], Loss: 0.1732\n",
      "Epoch [3700/10000], Loss: 0.1716\n",
      "Epoch [3800/10000], Loss: 0.1700\n",
      "Epoch [3900/10000], Loss: 0.1685\n",
      "Epoch [4000/10000], Loss: 0.1670\n",
      "Epoch [4100/10000], Loss: 0.1655\n",
      "Epoch [4200/10000], Loss: 0.1641\n",
      "Epoch [4300/10000], Loss: 0.1627\n",
      "Epoch [4400/10000], Loss: 0.1613\n",
      "Epoch [4500/10000], Loss: 0.1599\n",
      "Epoch [4600/10000], Loss: 0.1586\n",
      "Epoch [4700/10000], Loss: 0.1574\n",
      "Epoch [4800/10000], Loss: 0.1561\n",
      "Epoch [4900/10000], Loss: 0.1549\n",
      "Epoch [5000/10000], Loss: 0.1537\n",
      "Epoch [5100/10000], Loss: 0.1526\n",
      "Epoch [5200/10000], Loss: 0.1514\n",
      "Epoch [5300/10000], Loss: 0.1503\n",
      "Epoch [5400/10000], Loss: 0.1493\n",
      "Epoch [5500/10000], Loss: 0.1482\n",
      "Epoch [5600/10000], Loss: 0.1472\n",
      "Epoch [5700/10000], Loss: 0.1462\n",
      "Epoch [5800/10000], Loss: 0.1452\n",
      "Epoch [5900/10000], Loss: 0.1442\n",
      "Epoch [6000/10000], Loss: 0.1432\n",
      "Epoch [6100/10000], Loss: 0.1423\n",
      "Epoch [6200/10000], Loss: 0.1413\n",
      "Epoch [6300/10000], Loss: 0.1404\n",
      "Epoch [6400/10000], Loss: 0.1394\n",
      "Epoch [6500/10000], Loss: 0.1384\n",
      "Epoch [6600/10000], Loss: 0.1375\n",
      "Epoch [6700/10000], Loss: 0.1365\n",
      "Epoch [6800/10000], Loss: 0.1355\n",
      "Epoch [6900/10000], Loss: 0.1345\n",
      "Epoch [7000/10000], Loss: 0.1335\n",
      "Epoch [7100/10000], Loss: 0.1324\n",
      "Epoch [7200/10000], Loss: 0.1313\n",
      "Epoch [7300/10000], Loss: 0.1302\n",
      "Epoch [7400/10000], Loss: 0.1291\n",
      "Epoch [7500/10000], Loss: 0.1279\n",
      "Epoch [7600/10000], Loss: 0.1267\n",
      "Epoch [7700/10000], Loss: 0.1254\n",
      "Epoch [7800/10000], Loss: 0.1240\n",
      "Epoch [7900/10000], Loss: 0.1227\n",
      "Epoch [8000/10000], Loss: 0.1212\n",
      "Epoch [8100/10000], Loss: 0.1197\n",
      "Epoch [8200/10000], Loss: 0.1181\n",
      "Epoch [8300/10000], Loss: 0.1165\n",
      "Epoch [8400/10000], Loss: 0.1147\n",
      "Epoch [8500/10000], Loss: 0.1129\n",
      "Epoch [8600/10000], Loss: 0.1110\n",
      "Epoch [8700/10000], Loss: 0.1090\n",
      "Epoch [8800/10000], Loss: 0.1069\n",
      "Epoch [8900/10000], Loss: 0.1048\n",
      "Epoch [9000/10000], Loss: 0.1025\n",
      "Epoch [9100/10000], Loss: 0.1002\n",
      "Epoch [9200/10000], Loss: 0.0977\n",
      "Epoch [9300/10000], Loss: 0.0952\n",
      "Epoch [9400/10000], Loss: 0.0926\n",
      "Epoch [9500/10000], Loss: 0.0900\n",
      "Epoch [9600/10000], Loss: 0.0873\n",
      "Epoch [9700/10000], Loss: 0.0845\n",
      "Epoch [9800/10000], Loss: 0.0818\n",
      "Epoch [9900/10000], Loss: 0.0790\n",
      "Epoch [10000/10000], Loss: 0.0762\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# Train the model\n",
    "\n",
    "num_epochs = 10000\n",
    "models = {\n",
    "    1: model1,\n",
    "    2: model2,\n",
    "    3: model3,\n",
    "}\n",
    "optimizers = {\n",
    "    1: optimizer1,\n",
    "    2: optimizer2,\n",
    "    3: optimizer3,\n",
    "}\n",
    "for i in range(1,4):\n",
    "    print(\"Model \" + str(i))\n",
    "    for epoch in range(num_epochs):\n",
    "        output = models[i](data_in)\n",
    "\n",
    "        loss = criterion(output, data_target)\n",
    "\n",
    "        optimizers[i].zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers[i].step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dff3ec1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:10.141679Z",
     "end_time": "2023-04-01T19:43:10.142117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# visualize the resuts\n",
    "print(data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1a7518b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-01T19:43:10.141824Z",
     "end_time": "2023-04-01T19:43:10.142399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.50%\n",
      "Accuracy: 100.00%\n",
      "Accuracy: 100.00%\n",
      "Model 2 is the best!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print model wights\n",
    "accuracies = []\n",
    "models = [model1, model2, model3]\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data_in)\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        accuracy = (predicted == data_target).float().mean()\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy.item() * 100))\n",
    "\n",
    "# Select the best-performing model\n",
    "best_model = models[accuracies.index(max(accuracies))]\n",
    "\n",
    "print('Model ' + str(accuracies.index(max(accuracies)) + 1) + ' is the best!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
